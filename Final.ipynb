{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "9e49a640", "cell_type": "markdown", "source": "# Create SageMaker Pipelines from a data flow\n\nYou can use Amazon SageMaker Pipelines to create end-to-end workflows that manage and deploy SageMaker jobs. \nPipelines come with SageMaker Python SDK integration, so you can build each step of your workflow using a \nPython-based interface.\n\nThis notebook create a SageMaker pipeline that executes your data flow `Final.flow` on the \nentire dataset as a data preparation step and optionally you can add additional steps to the pipeline.\nYou will execute the pipeline and monitor its status using SageMaker Pipeline APIs.\n\nThe pipeline will be processing data from the step `Custom Code` (**Step Output Name: default**). To save from a different step, go to Canvas \nto select a new step to export. After your workflow is deployed, you can view the Directed Acyclic Graph\n(DAG) for your pipeline and manage your executions using Amazon SageMaker Studio.", "metadata": {}}, {"id": "23d9d530", "cell_type": "markdown", "source": "# Install SageMaker Autopilot Dependencies\nTo use SageMaker Autopilot in your pipeline, you must install the newest versions of botocore, boto3, and sagemaker.", "metadata": {}}, {"id": "7d8bac57", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install -U 'sagemaker>=2.118.0'", "outputs": []}, {"id": "56ffee03", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import sagemaker\nfrom packaging import version\n\nif version.parse(sagemaker.__version__) >= version.parse(\"2.118.0\"):\n    automl_enabled = True\nelse:\n    automl_enabled = False\n    print(f\"The AutoML pipeline step requires sagemaker >= 2.118.0, but you have version {sagemaker.__version__}. The notebook uses the XGBoost step for training instead. To use the AutoML step, rerun the preceding cell and make sure the that you're using sagemaker 2.118.0 or later.\")", "outputs": []}, {"id": "d3af71f6", "cell_type": "markdown", "source": "# Inputs and Outputs\n\nThe below settings configure the inputs and outputs for the flow export.\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Configurable Settings </strong>\n\nIn <b>Input - Source</b> you can configure the data sources that will be used as input by Canvas\n\n1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n2. For all other sources, configure attributes like query_string, database in the source's \n<b>DatasetDefinition</b> object.\n\nIf you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \nYou should also re-execute the cells in this section if you have modified the settings in any data sources.\n\nParametrized data sources will be ignored when creating ProcessingInputs, and will directly read from the source.\nNetwork isolation is not supported for parametrized data sources.\n</div>", "metadata": {}}, {"id": "9484c78a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n\ndata_sources = []", "outputs": []}, {"id": "d38c481f", "cell_type": "markdown", "source": "## Output: S3 settings\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Configurable Settings </strong>\n\n1. <b>bucket</b>: you can configure the S3 bucket where Canvas will save the output. The default bucket from \nthe SageMaker notebook session is used. \n2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \nconflict with other flow exports \n3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n</div>", "metadata": {}}, {"id": "e5199a12", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import time\nimport uuid\nimport boto3\nimport sagemaker\n\n# Sagemaker session\nsess = sagemaker.Session()\n\nregion = boto3.Session().region_name\n\n# You can configure this with your own bucket name, e.g.\n# bucket = \"my-bucket\"\nbucket = sess.default_bucket()\nprint(f\"Canvas export storage bucket: {bucket}\")\n\n# unique flow export ID\nflow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\nflow_export_name = f\"flow-{flow_export_id}\"", "outputs": []}, {"id": "36617b82", "cell_type": "markdown", "source": "Below are the inputs required by the SageMaker Python SDK to launch a processing job.", "metadata": {}}, {"id": "741856a8", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Output name is auto-generated from the select node's ID + output name from the flow file.\noutput_name = \"2aa3fdbf-cf55-459f-b87b-94c205f7dc5e.default\"\n\ns3_output_prefix = f\"export-{flow_export_name}/output\"\ns3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\nprint(f\"Processing output base path: {s3_output_base_path}\\nThe final output location will contain additional subdirectories.\")\n\nprocessing_job_output = ProcessingOutput(\n    output_name=output_name,\n    source=\"/opt/ml/processing/output\",\n    destination=s3_output_base_path,\n    s3_upload_mode=\"EndOfJob\"\n)", "outputs": []}, {"id": "ad7e35ee", "cell_type": "markdown", "source": "## Upload Flow to S3\n\nTo use the data flow as an input to the processing job, first upload your flow file to Amazon S3.", "metadata": {}}, {"id": "5986fc9a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import os\nimport json\nimport boto3\n\n# name of the flow file which should exist in the current notebook working directory\nflow_file_name = \"Final.flow\"\n\n# Load .flow file from current notebook working directory \n!echo \"Loading flow file from current notebook working directory: $PWD\"\n\nwith open(flow_file_name) as f:\n    flow = json.load(f)\n\n# Upload flow to S3\ns3_client = boto3.client(\"s3\")\ns3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n\nflow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_export_name}.flow\"\n\nprint(f\"Data flow {flow_file_name} uploaded to {flow_s3_uri}\")", "outputs": []}, {"id": "baf0c151", "cell_type": "markdown", "source": "The data flow is also provided to the Processing Job as an input source which we configure below.", "metadata": {}}, {"id": "b6a48eef", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Input - Flow: Final.flow\nflow_input = ProcessingInput(\n    source=flow_s3_uri,\n    destination=\"/opt/ml/processing/flow\",\n    input_name=\"flow\",\n    s3_data_type=\"S3Prefix\",\n    s3_input_mode=\"File\",\n    s3_data_distribution_type=\"FullyReplicated\"\n)", "outputs": []}, {"id": "81be2135", "cell_type": "markdown", "source": "## Create Processor", "metadata": {}}, {"id": "738b5a25", "cell_type": "code", "metadata": {"tags": ["parameters"]}, "execution_count": null, "source": "from sagemaker import image_uris\n\n# IAM role for executing the processing job.\niam_role = sagemaker.get_execution_role()\n\n# Unique processing job name. Give a unique name every time you re-execute processing jobs.\nprocessing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n\n# Canvas Container URL.\ncontainer_uri = \"599662218115.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-data-wrangler-container:4.x\"\n# Pinned Canvas Container URL.\ncontainer_uri_pinned = \"599662218115.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-data-wrangler-container:4.6.9\"\n\n# Processing Job Instance count and instance type.\ninstance_count = 2\ninstance_type = \"ml.m5.4xlarge\"\n\n# Size in GB of the EBS volume to use for storing data during processing.\nvolume_size_in_gb = 30\n\n\n# Content type for each output. Canvas supports CSV as default and Parquet.\noutput_content_type = \"CSV\"\n\n# Delimiter to use for the output if the output content type is CSV. Uncomment to set.\n# delimiter = \",\"\n\n# Compression to use for the output. Uncomment to set.\n# compression = \"gzip\"\n\n# Configuration for partitioning the output. Uncomment to set.\n# \"num_partition\" sets the number of partitions/files written in the output.\n# \"partition_by\" sets the column names to partition the output by.\n# partition_config = {\n#     \"num_partitions\": 1,\n#     \"partition_by\": [\"column_name_1\", \"column_name_2\"],\n# }\n\n# Network Isolation mode; default is off.\nenable_network_isolation = False\n\n# List of tags to be passed to the processing job.\nuser_tags = None\n\n# Output configuration used as processing job container arguments. Only applies when writing to S3.\n# Uncomment to set additional configurations.\noutput_config = {\n    output_name: {\n        \"content_type\": output_content_type,\n        # \"delimiter\": delimiter,\n        # \"compression\": compression,\n        # \"partition_config\": partition_config,\n    }\n}\n\n# Refit configuration determines whether Canvas refits the trainable parameters on the entire dataset. \n# When True, the processing job relearns the parameters and outputs a new flow file.\n# You can specify the name of the output flow file under 'output_flow'.\n# Note: There are length constraints on the container arguments (max 256 characters).\nuse_refit = False\nrefit_trained_params = {\n    \"refit\": use_refit,\n    \"output_flow\": f\"data-wrangler-flow-processing-{flow_export_id}.flow\"\n}\n\n# KMS key for per object encryption; default is None.\nkms_key = None\n\n# Inference parameters determine whether Canvas generates an inference artifact at the end of the job run.\n# When set, the processing job generates an inference artifact and uploads it to S3 under the S3 prefix of `flow_s3_uri`.\n# You can specify the name of the output artifact with 'inference_artifact_name'.\nuse_inference_params = False\ninference_artifact_name = f\"data-wrangler-flow-processing-{flow_export_id}.tar.gz\"\ninference_params = {\n    \"inference_artifact_name\": inference_artifact_name,\n    \"output_node_id\": output_name.split(\".\")[0]\n}", "outputs": []}, {"id": "01e187f1", "cell_type": "markdown", "source": "### (Optional) Configure Spark Cluster Driver Memory", "metadata": {}}, {"id": "8a48c818", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# The Spark memory configuration. Change to specify the driver and executor memory in MB for the Spark cluster during processing.\ndriver_memory_in_mb = 55742\nexecutor_memory_in_mb = 55742\n\nconfig = json.dumps({\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n        \"spark.driver.memory\": f\"{driver_memory_in_mb}m\",\n        \"spark.executor.memory\": f\"{executor_memory_in_mb}m\"\n    }\n})\n\n# Provides the spark config file to processing job and set the cluster driver memory. Uncomment to set.\n# config_file = f\"config-{flow_export_id}.json\"\n# with open(config_file, \"w\") as f:\n#     f.write(config)\n\n# config_s3_path = f\"spark_configuration/{processing_job_name}/configuration.json\"\n# config_s3_uri = f\"s3://{bucket}/{config_s3_path}\"\n# s3_client.upload_file(config_file, bucket, config_s3_path, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n# print(f\"Spark Config file uploaded to {config_s3_uri}\")\n# os.remove(config_file)\n\n# data_sources.append(ProcessingInput(\n#     source=config_s3_uri,\n#     destination=\"/opt/ml/processing/input/conf\",\n#     input_name=\"spark-config\",\n#     s3_data_type=\"S3Prefix\",\n#     s3_input_mode=\"File\",\n#     s3_data_distribution_type=\"FullyReplicated\"\n# ))", "outputs": []}, {"id": "90d98cbe", "cell_type": "markdown", "source": "## Create Processer for the Pipeline\n\nCreate a Processor object. The pipeline uses the processor to apply the transformations from your flow file to the dataset.\n", "metadata": {}}, {"id": "af31828d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Setup processing job network configuration\nfrom sagemaker.network import NetworkConfig\n\nnetwork_config = NetworkConfig(\n    enable_network_isolation=enable_network_isolation,\n    security_group_ids=['sg-0fcba015345bc1441'],\n    subnets=['subnet-0f9140916caebc5f7', 'subnet-0c432a4f4028d0cb6', 'subnet-036ceb6cbf89d4784']\n)", "outputs": []}, {"id": "6d53ff0c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.processing import Processor\n\nprocessor = Processor(\n    role=iam_role,\n    image_uri=container_uri,\n    instance_count=instance_count,\n    instance_type=instance_type,\n    volume_size_in_gb=volume_size_in_gb,\n    network_config=network_config,\n    sagemaker_session=sess,\n    output_kms_key=kms_key,\n    tags=user_tags\n)", "outputs": []}, {"id": "1ad6857f", "cell_type": "markdown", "source": "# Create SageMaker Pipeline\nA SageMaker pipeline is composed of a series of steps. You begin by creating a processing step to \ntransform your data. If you\u2019re using the notebook to train a model, you also define a training step. \n\n## Define Pipeline Steps\nTo create a SageMaker pipeline, create a `ProcessingStep` using the Canvas processor defined \nin the preceding section.", "metadata": {}}, {"id": "cb1f7880", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "job_arguments = [f\"--output-config '{json.dumps(output_config)}'\"]\nif use_refit:\n    job_arguments.append(f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\")\nif use_inference_params:\n    job_arguments.append(f\"--inference-params '{json.dumps(inference_params)}'\")", "outputs": []}, {"id": "721c1e0d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.workflow.steps import ProcessingStep\n\ndata_wrangler_step = ProcessingStep(\n    name=\"DataWranglerProcessingStep\",\n    processor=processor,\n    inputs=[flow_input] + data_sources, \n    outputs=[processing_job_output],\n    job_arguments=job_arguments,\n)", "outputs": []}, {"id": "a06fd9c5", "cell_type": "markdown", "source": "You can add a `TrainingStep` to the pipeline. The step trains a model on the transformed dataset. By default, the notebook does not add a training step. To add a training step, set `add_training_step` to True.\nIf you add a training step, you can choose between an AutoML training step and an XGBoost training step.\nYou can also add more steps. For information about adding steps to a pipeline, see [Define a Pipeline](http://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html).", "metadata": {}}, {"id": "0141d758", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "add_training_step = False", "outputs": []}, {"id": "8814f8f2", "cell_type": "markdown", "source": "To use the AutoML training step, run the following cells as-is. To use the XGBoost training step, set `use_automl_step` to False.\n\nCurrently, the AutoML training step only supports the `ENSEMBLING` mode. For more information about the AutoML training step, \nsee [Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-automl).", "metadata": {}}, {"id": "75a04f2c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "use_automl_step = automl_enabled", "outputs": []}, {"id": "aa288791", "cell_type": "markdown", "source": "For AutoML, specifying the target column of your dataset is required. \nThe target column is the column that the model is trained to predict. \nProvide the name of the target column in the following cell.", "metadata": {}}, {"id": "07bbd567", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "target_attribute_name = \"\"  # Provide the target column name here\n\nif use_automl_step and not target_attribute_name:\n    raise RuntimeError(\"You must specify the target column name.\")", "outputs": []}, {"id": "82f415ab", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "if add_training_step and use_automl_step:\n    from sagemaker import AutoML, AutoMLInput\n    from sagemaker.workflow.automl_step import AutoMLStep\n    from sagemaker.workflow.functions import Join\n    from sagemaker.workflow.pipeline_context import PipelineSession\n\n    pipeline_session = PipelineSession()\n\n    training_input_content_type = None\n\n    if output_content_type == \"CSV\":\n        training_input_content_type = 'text/csv;header=present'\n    elif output_content_type == \"Parquet\":\n        training_input_content_type = 'x-application/vnd.amazon+parquet'\n\n    auto_ml = AutoML(\n        role=iam_role,\n        target_attribute_name=target_attribute_name,\n        sagemaker_session=pipeline_session,\n        mode=\"ENSEMBLING\"\n    )\n\n    s3_input = Join(\n        on=\"/\",\n        values=[\n            data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n            data_wrangler_step.properties.ProcessingJobName,\n            f'{output_name.replace(\".\", \"/\")}',\n        ]\n    )\n\n    train_args = auto_ml.fit(\n        inputs=AutoMLInput(\n            inputs=s3_input,\n            content_type=training_input_content_type,\n            target_attribute_name=target_attribute_name\n        )\n    )\n\n    training_step = AutoMLStep(\n        name=\"DataWrangerAutoML\",\n        step_args=train_args,\n    )", "outputs": []}, {"id": "450ed297", "cell_type": "markdown", "source": "The XGBoost algorithm uses the first column as the target column. If this is not the case for \nthe data that you\u2019ve processed, use the \"Move column\" transform to make the target column the \nfirst column.", "metadata": {}}, {"id": "badbf655", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "if add_training_step and not use_automl_step:\n    from sagemaker.estimator import Estimator\n    from sagemaker.workflow.functions import Join\n\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"xgboost\",\n        region=region,\n        version=\"1.5-1\",\n        py_version=\"py3\",\n        instance_type=instance_type,\n    )\n    xgb_train = Estimator(\n        image_uri=image_uri,\n        instance_type=instance_type,\n        instance_count=1,\n        role=iam_role,\n    )\n    xgb_train.set_hyperparameters(\n        objective=\"reg:squarederror\",\n        num_round=3,\n    )\n\n    from sagemaker.inputs import TrainingInput\n    from sagemaker.workflow.steps import TrainingStep\n\n    xgb_input_content_type = None\n\n    if output_content_type == \"CSV\":\n        xgb_input_content_type = 'text/csv'\n    elif output_content_type == \"Parquet\":\n        xgb_input_content_type = 'application/x-parquet'\n\n    training_step = TrainingStep(\n        name=\"DataWrangerTrain\",\n        estimator=xgb_train,\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=Join(\n                    on=\"/\",\n                    values=[\n                        data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n                        data_wrangler_step.properties.ProcessingJobName,\n                        f'{output_name.replace(\".\", \"/\")}',\n                    ]\n                ),\n                content_type=xgb_input_content_type\n            )\n        }\n    )", "outputs": []}, {"id": "28d8d20e", "cell_type": "markdown", "source": "## Define a Pipeline of Parameters, Steps\nNow you will create the SageMaker pipeline that combines the steps created above so it can be executed. \n\nDefine Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n\nThe parameters supported in this notebook includes:\n\n- `instance_type` - The ml.* instance type of the processing job.\n- `instance_count` - The instance count of the processing job.", "metadata": {}}, {"id": "2a2806d4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.workflow.parameters import (\n    ParameterInteger,\n    ParameterString,\n)\n# Define Pipeline Parameters\ninstance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\ninstance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)", "outputs": []}, {"id": "5a7250ee", "cell_type": "markdown", "source": "You will create a pipeline with the steps and parameters defined above", "metadata": {}}, {"id": "ad29b968", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import time\nimport uuid\n\nfrom sagemaker.workflow.pipeline import Pipeline\n\n# Create a unique pipeline name with flow export name\npipeline_name = f\"pipeline-{flow_export_name}\"\n\n# Combine pipeline steps\npipeline_steps = [data_wrangler_step]\nif add_training_step:\n    pipeline_steps.append(training_step)\n\npipeline = Pipeline(\n    name=pipeline_name,\n    parameters=[instance_type, instance_count],\n    steps=pipeline_steps,\n    sagemaker_session=sess\n)", "outputs": []}, {"id": "873c671f", "cell_type": "markdown", "source": "### (Optional) Examining the pipeline definition\n\nThe JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and \nthe parameters and step properties resolve correctly.", "metadata": {}}, {"id": "6a09944d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import json\n\ndefinition = json.loads(pipeline.definition())\ndefinition", "outputs": []}, {"id": "b81b0769", "cell_type": "markdown", "source": "## Submit the pipeline to SageMaker and start execution\n\nSubmit the pipeline definition to the SageMaker Pipeline service and start an execution. The role passed in \nwill be used by the Pipeline service to create all the jobs defined in the steps.", "metadata": {}}, {"id": "0ad606d6", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "pipeline.upsert(role_arn=iam_role)\nexecution = pipeline.start()", "outputs": []}, {"id": "90eae8d0", "cell_type": "markdown", "source": "## Pipeline Operations: Examine and Wait for Pipeline Execution\n\nDescribe the pipeline execution and wait for its completion.", "metadata": {}}, {"id": "b3ff0b37", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "try:\n    # Waiter will wait for up to 1 hour; increase the delay and max_attempts if necessary\n    execution.wait(delay=30, max_attempts=120)\nexcept Exception as e:\n    listed_steps = execution.list_steps()\n    errors = []\n    for step in listed_steps:\n        if \"FailureReason\" in step:\n            errors.append(step[\"FailureReason\"])\n    raise RuntimeError(str(errors)) from e", "outputs": []}, {"id": "31ea548c", "cell_type": "markdown", "source": "List the steps in the execution. These are the steps in the pipeline that have been resolved by the step \nexecutor service.", "metadata": {}}, {"id": "38cdcffe", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "execution.list_steps()", "outputs": []}, {"id": "88340d47", "cell_type": "markdown", "source": "You can visualize the pipeline execution status and details in Studio. For details please refer to \n[View, Track, and Execute SageMaker Pipelines in SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-studio.html)", "metadata": {}}, {"id": "65e82dd8", "cell_type": "markdown", "source": "## Canvas Step S3 Output Location\nThe output of your Canvas processing step will be printed below. To prevent data of different processing jobs \nand different output nodes from being overwritten or combined, Canvas uses the name of the processing job and \nthe name of the output to write the output.", "metadata": {}}, {"id": "359cfe19", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.processing import ProcessingJob\n\ndata_wrangler_job_arn = execution.list_steps()[-1][\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\ndata_wrangler_job = ProcessingJob.from_processing_arn(sess, data_wrangler_job_arn)\ndata_wrangler_job_name = data_wrangler_job.describe()[\"ProcessingJobName\"]\n\ns3_job_results_path = f\"{s3_output_base_path}/{data_wrangler_job_name}/{output_name.replace('.', '/')}\"\nprint(f\"Job results are saved to S3 path: {s3_job_results_path}\")", "outputs": []}, {"id": "ed8959eb", "cell_type": "markdown", "source": "### (Optional) Load Processed Data into Pandas\n\nWe use the [AWS SDK for pandas library](https://github.com/awslabs/aws-sdk-pandas) to load the exported \ndataset into a Pandas data frame for a preview of first 10000 rows.\n\nTo turn on automated visualizations and data insights for your pandas data frame, import the sagemaker_datawrangler library.", "metadata": {}}, {"id": "fc21826f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install -q awswrangler pandas\nimport awswrangler as wr\n\n# Import sagemaker_datawrangler to show visualizations and automated data\n# quality insights, and export code to prepare data in a pandas data frame.\ntry:\n    import sagemaker_datawrangler\nexcept ImportError:\n    print(\"sagemaker_datawrangler is not imported. Change your kernel to the Data Science 3.0 Kernel Image and try again.\")\n    pass", "outputs": []}, {"id": "d61bc549", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "chunksize = 10000\n\nif output_content_type.upper() == \"CSV\":\n    dfs = wr.s3.read_csv(s3_job_results_path, chunksize=chunksize)\nelif output_content_type.upper() == \"PARQUET\":\n    dfs = wr.s3.read_parquet(s3_job_results_path, chunked=chunksize)\nelse:\n    print(f\"Unexpected output content type {output_content_type}\") \n\ndf = next(dfs)\ndf", "outputs": []}, {"id": "c2593a56", "cell_type": "markdown", "source": "## (Optional) Pipeline cleanup\nTo delete the SageMaker resources created in this notebook, set `pipeline_deletion` to True.", "metadata": {}}, {"id": "802fec8f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "pipeline_deletion = False", "outputs": []}, {"id": "b8b4831e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "if pipeline_deletion:\n    pipeline.delete()", "outputs": []}]}